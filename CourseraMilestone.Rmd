---
title: "Coursera Capstone Milestone"
author: "wpmcdonald"
date: "March 21, 2015"
output: html_document
---

# Introduction
This project is an investigation into using data scince in the area of Natual Language Processing and Text Mining

Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation. Wikipedia

Text Mining refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. 
Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others. Wikipedia: https://en.wikipedia.org/wiki/Text_mining

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. 

The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 

# Data Acquisition
Sample text files were provided by SwiftKey in English, German, and Finnish, and Russion
We will only deal with English version of the supplied data files

 * Download the data from "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
 * Import into RStudio
 * Sample the data
 * Tokenize
 * Profanity Filtering


```{r LoadData}
setwd( "/Users/williammcdonald/CourseraCapstoneData/")
load('/Users/williammcdonald/CourseraCapstoneData/twitter.RData')
load('/Users/williammcdonald/CourseraCapstoneData/news.RData')
load('/Users/williammcdonald/CourseraCapstoneData/blogs.RData')
naughty <- readLines("swearWords.txt", skipNul = TRUE)
```

Check the size and type of files

```{r, CleanText echo=FALSE}
library(tm)
library(RWeka)
library(SnowballC)
# source("/Users/williammcdonald/Coursera-JHU-Capstone/Capstone_helper.R")
cleanText <- function(text){
        # remove all hashtags, substitute for profanity, remove special characters 
        x <- gsub(hashtags, " ", text)
        x <- gsub(paste0('\\<', naughty , '\\>', collapse = '|'), '!$?%', x)
        x <- gsub(paste0(special, collapse = '|'), " ", x) 
        # Clean text
        x <- tolower(x)
        x <- stripWhitespace(x)
        x <- removePunctuation(x, preserve_intra_word_dashes = TRUE)
        x <= removeNumbers(x)
        x <- strsplit(x, "\\W")
        return(x)
}
hashtags <- "#[0-9][a-z][A-Z]+"
special <- c("®","™", "¥", "£", "¢", "€", "#")
twitter <- cleanText(twitter)
news <- cleanText(news)
blogs <- cleanText(blogs)

twit.char <- sapply(twitter, nchar)
twit.charlen <- sapply(twit.char, sum)
twit.wordlen <- sapply(twit.char, length)

news.char <- sapply(news, nchar)
news.charlen <- sapply(news.char, sum)
news.wordlen <- sapply(news.char, length)

blogs.char <- sapply(blogs, nchar)
blogs.charlen <- sapply(blogs.char, sum)
blogs.wordlen <- sapply(blogs.char, length)
```

Basic summary file information. Boxplots of number of words and characters per sample file

```{r}
par(mfrow = 2, mfcol = 3)
boxplot(twit.wordlen, main = "Words per Tweet")
boxplot(news.wordlen, main = "Words per News Item")
boxplot(blogs.wordlen, main = "Words per Blogs Item")
boxplot(twit.charlen, main = "Characters per Tweet")
boxplot(news.charlen, main = "Characters per News Item")
boxplot(blogs.charlen, main = "Characters per Blog Item")


# twitl <- length(twitter)
# newsl <- length(news)
# blogl <- length(blogs)
# 
# twitc <- sum(nchar(twitter))
# newsc <- sum(nchar(news))
# blogc <- sum(nchar(blogs))
# 
# twitmax <- max(nchar(twitter))
# newsmax <- max(nchar(news))
# blogmax <- max(nchar(blogs))
# 
# twitinfo <- twitc/twitl
# newsinfo <- newsc/newsl
# bloginfo <- blogc/blogl

```

# Sampling

```{r plots }
# barplot(c(twitl, newsl, blogl), main = "Lines per file", ylab = 'lines', names.arg = c("Twitter", "News", "Blogs"), col = 404)
# 
# barplot(c(twitmax, newsmax, blogmax), main = "Max characters per line", ylab = "characters", names.arg = c("Twitter", "News", "Blogs"), col = 54)
# 
# barplot(c(twitinfo, newsinfo, bloginfo), main = 'Characters per line', ylab = "characters", names.arg = c("twitter", "news", "blogs"), col = 254)
```

Based on the initial exploration, the files are between 900,000 and 2.4 million lines with a maximum number of charaacters per line between 140 (twitter) and 40,000 (blogs). Based on the size of the files, number of lines, and the probable total number of words, analysis of the entire data set will be handled by sampling and statistical inference. 

## Sampling the Data Sets Individually
We'll start initial sampling at 1% which corresponds to 23,600 tweets, 10,000 news items, and 8993 blog entries 

```{r sampling}
twit.smpl <- sample(twitter, size = round(length(twitter)/100))
news.smpl <- sample(news, size = round(length(news)/100))
blog.smpl <- sample(blogs, size = round(length(blogs)/100))
```


